{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0b7511",
   "metadata": {},
   "source": [
    "This network is to test code done for Project 2. \n",
    "Go down to the 'full prediction' heading to check out the model in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c260acfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x111d7bdc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import prologue as prologue\n",
    "import framework\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(True) #By the end we should have this set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69915a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block we discuss put all the external functions, \n",
    "# namely generate_disc, computing number of errors, and training the model.\n",
    "# We should not need to change any of this.\n",
    "\n",
    "######################################################################\n",
    "######################### DATA things ################################\n",
    "######################################################################\n",
    "def generate_disc_set(nb): # This can be kept the same\n",
    "    input = torch.empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(100)\n",
    "test_input, test_target = generate_disc_set(100)\n",
    "\n",
    "# center the data:\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 1\n",
    "\n",
    "######################################################################\n",
    "######################### ERROR things ################################\n",
    "######################################################################\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target): #this can be kept the same\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################### TRAIN things ################################\n",
    "######################################################################\n",
    "# The following should be modified\n",
    "\n",
    "def train_model(model, train_input, train_target):\n",
    "    criterion = nn.CrossEntropyLoss() #replace this by self macde MSE\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1) #replace this by self made SGD for MSE\n",
    "    nb_epochs = 10\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad() # replace this so it works with \n",
    "            loss.backward() # replace this to self made backward method\n",
    "            optimizer.step() # '_'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61590875",
   "metadata": {},
   "source": [
    "# full prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553539e8",
   "metadata": {},
   "source": [
    "Now we try to do a full prediction task with one fully connected linear layer, and one relu nonlin, and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e33da45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function. Not sure how useful it is\n",
    "def modif_target(val):\n",
    "    if val==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bcfb1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our model. The last output is our \"prediction\" which we compute the loss  with.\n",
    "model = framework.Sequential((framework.Linear(2, 50), \n",
    "                              framework.ReLu(), \n",
    "                              framework.Linear(50, 25), \n",
    "                              framework.ReLu(),\n",
    "                              framework.Linear(25, 1)))\n",
    "loss = framework.MSE() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4bfa142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([108.3281])\n"
     ]
    }
   ],
   "source": [
    "# first compute the total loss on the first inputs:\n",
    "loss_acc = 0\n",
    "for i in range(100):\n",
    "    loss_acc += loss.forward(model.forward(train_input[i]), modif_target(train_target[i]))\n",
    "print(loss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3608d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training on the first samples. We run backward by flowing back the derivative of the loss. \n",
    "# Whenever the backward method is ran, each layer stores an updated gradient. \n",
    "for i in range(100):\n",
    "    model.backward(loss.backward(model.forward(train_input[i]), modif_target(train_target[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c9c1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then perform an optimization step. This updates the weights by using the stored gradient.\n",
    "model.SGD_step(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5e13fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "#Here we compute the accuracy:\n",
    "tot_right=0\n",
    "for i in range(100):\n",
    "    #print(model.forward(train_input[i]))\n",
    "    #print(train_target[i])\n",
    "    if model.forward(train_input[i]) <0.0 and  train_target[i]==0:\n",
    "        tot_right+=1\n",
    "    elif model.forward(train_input[i]) >=0.0 and  train_target[i]==1:\n",
    "        tot_right+=1\n",
    "print(tot_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95c7d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh nb:  0 loss:  tensor([342.7868])\n",
      "epcoh nb:  1 loss:  tensor([264.6042])\n",
      "epcoh nb:  2 loss:  tensor([263.1952])\n",
      "epcoh nb:  3 loss:  tensor([262.3332])\n",
      "epcoh nb:  4 loss:  tensor([261.6960])\n",
      "epcoh nb:  5 loss:  tensor([261.1650])\n",
      "epcoh nb:  6 loss:  tensor([260.6961])\n",
      "epcoh nb:  7 loss:  tensor([260.2710])\n",
      "epcoh nb:  8 loss:  tensor([259.8813])\n",
      "epcoh nb:  9 loss:  tensor([259.5216])\n",
      "epcoh nb:  10 loss:  tensor([259.1881])\n",
      "epcoh nb:  11 loss:  tensor([258.8783])\n",
      "epcoh nb:  12 loss:  tensor([258.5893])\n",
      "epcoh nb:  13 loss:  tensor([258.3192])\n",
      "epcoh nb:  14 loss:  tensor([258.0660])\n",
      "epcoh nb:  15 loss:  tensor([257.8282])\n",
      "epcoh nb:  16 loss:  tensor([257.6040])\n",
      "epcoh nb:  17 loss:  tensor([257.3917])\n",
      "epcoh nb:  18 loss:  tensor([257.1908])\n",
      "epcoh nb:  19 loss:  tensor([256.9999])\n",
      "epcoh nb:  20 loss:  tensor([256.8181])\n",
      "epcoh nb:  21 loss:  tensor([256.6445])\n",
      "epcoh nb:  22 loss:  tensor([256.4781])\n",
      "epcoh nb:  23 loss:  tensor([256.3186])\n",
      "epcoh nb:  24 loss:  tensor([256.1655])\n",
      "epcoh nb:  25 loss:  tensor([256.0170])\n",
      "epcoh nb:  26 loss:  tensor([255.8739])\n",
      "epcoh nb:  27 loss:  tensor([255.7352])\n",
      "epcoh nb:  28 loss:  tensor([255.6003])\n",
      "epcoh nb:  29 loss:  tensor([255.4687])\n",
      "epcoh nb:  30 loss:  tensor([255.3404])\n",
      "epcoh nb:  31 loss:  tensor([255.2146])\n",
      "epcoh nb:  32 loss:  tensor([255.0912])\n",
      "epcoh nb:  33 loss:  tensor([254.9690])\n",
      "epcoh nb:  34 loss:  tensor([254.8492])\n",
      "epcoh nb:  35 loss:  tensor([254.7300])\n",
      "epcoh nb:  36 loss:  tensor([254.6116])\n",
      "epcoh nb:  37 loss:  tensor([254.4940])\n",
      "epcoh nb:  38 loss:  tensor([254.3761])\n",
      "epcoh nb:  39 loss:  tensor([254.2580])\n",
      "epcoh nb:  40 loss:  tensor([254.1393])\n",
      "epcoh nb:  41 loss:  tensor([254.0198])\n",
      "epcoh nb:  42 loss:  tensor([253.8988])\n",
      "epcoh nb:  43 loss:  tensor([253.7756])\n",
      "epcoh nb:  44 loss:  tensor([253.6507])\n",
      "epcoh nb:  45 loss:  tensor([253.5225])\n",
      "epcoh nb:  46 loss:  tensor([253.3914])\n",
      "epcoh nb:  47 loss:  tensor([253.2567])\n",
      "epcoh nb:  48 loss:  tensor([253.1177])\n",
      "epcoh nb:  49 loss:  tensor([252.9735])\n",
      "epcoh nb:  50 loss:  tensor([252.8242])\n",
      "epcoh nb:  51 loss:  tensor([252.6686])\n",
      "epcoh nb:  52 loss:  tensor([252.5060])\n",
      "epcoh nb:  53 loss:  tensor([252.3355])\n",
      "epcoh nb:  54 loss:  tensor([252.1558])\n",
      "epcoh nb:  55 loss:  tensor([251.9663])\n",
      "epcoh nb:  56 loss:  tensor([251.7661])\n",
      "epcoh nb:  57 loss:  tensor([251.5534])\n",
      "epcoh nb:  58 loss:  tensor([251.3270])\n",
      "epcoh nb:  59 loss:  tensor([251.0852])\n",
      "epcoh nb:  60 loss:  tensor([250.8264])\n",
      "epcoh nb:  61 loss:  tensor([250.5485])\n",
      "epcoh nb:  62 loss:  tensor([250.2493])\n",
      "epcoh nb:  63 loss:  tensor([249.9265])\n",
      "epcoh nb:  64 loss:  tensor([249.5771])\n",
      "epcoh nb:  65 loss:  tensor([249.1984])\n",
      "epcoh nb:  66 loss:  tensor([248.7868])\n",
      "epcoh nb:  67 loss:  tensor([248.3391])\n",
      "epcoh nb:  68 loss:  tensor([247.8512])\n",
      "epcoh nb:  69 loss:  tensor([247.3183])\n",
      "epcoh nb:  70 loss:  tensor([246.7365])\n",
      "epcoh nb:  71 loss:  tensor([246.1011])\n",
      "epcoh nb:  72 loss:  tensor([245.4073])\n",
      "epcoh nb:  73 loss:  tensor([244.6507])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-b10ed47cdf40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DeepLearning/deeplearningproj/Jacob/Project2/framework.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DeepLearning/deeplearningproj/Jacob/Project2/framework.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Tanh doesn't have any params so we don't need to update anything in gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#Do i need to initialize anything?? s is the input and x is output of activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m   \u001b[0;31m#need to save s to get dsigma and dl_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this is our model. The last output is our \"prediction\" which we compute the loss  with.\n",
    "model = framework.Sequential((framework.Linear(2, 10), \n",
    "                              framework.Tanh(), \n",
    "                              framework.Linear(10, 10), \n",
    "                              framework.Tanh(),\n",
    "                              framework.Linear(10, 50), \n",
    "                              framework.Tanh(),\n",
    "                              framework.Linear(50, 1)))\n",
    "loss = framework.MSE() \n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "nb_epochs = 100\n",
    "batch_size = 50\n",
    "for e in range(nb_epochs):\n",
    "    loss_acc=0\n",
    "    for b in range(0, 1000, batch_size):\n",
    "        for i in range(batch_size):\n",
    "            prediction = model.forward(train_input[b+i])\n",
    "            loss_acc += loss.forward(prediction, train_target[b+i])\n",
    "            model.backward(loss.backward(prediction, train_target[b+i]))\n",
    "        model.SGD_step(0.0005)\n",
    "    print('epcoh nb: ', e, 'loss: ', loss_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b47a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh nb:  0 loss:  tensor([35.7954])\n",
      "epcoh nb:  1 loss:  tensor([35.7946])\n",
      "epcoh nb:  2 loss:  tensor([35.7937])\n",
      "epcoh nb:  3 loss:  tensor([35.7928])\n",
      "epcoh nb:  4 loss:  tensor([35.7920])\n",
      "epcoh nb:  5 loss:  tensor([35.7911])\n",
      "epcoh nb:  6 loss:  tensor([35.7903])\n",
      "epcoh nb:  7 loss:  tensor([35.7894])\n",
      "epcoh nb:  8 loss:  tensor([35.7885])\n",
      "epcoh nb:  9 loss:  tensor([35.7876])\n",
      "epcoh nb:  10 loss:  tensor([35.7868])\n",
      "epcoh nb:  11 loss:  tensor([35.7859])\n",
      "epcoh nb:  12 loss:  tensor([35.7851])\n",
      "epcoh nb:  13 loss:  tensor([35.7842])\n",
      "epcoh nb:  14 loss:  tensor([35.7834])\n",
      "epcoh nb:  15 loss:  tensor([35.7825])\n",
      "epcoh nb:  16 loss:  tensor([35.7816])\n",
      "epcoh nb:  17 loss:  tensor([35.7808])\n",
      "epcoh nb:  18 loss:  tensor([35.7799])\n",
      "epcoh nb:  19 loss:  tensor([35.7791])\n",
      "epcoh nb:  20 loss:  tensor([35.7782])\n",
      "epcoh nb:  21 loss:  tensor([35.7774])\n",
      "epcoh nb:  22 loss:  tensor([35.7765])\n",
      "epcoh nb:  23 loss:  tensor([35.7756])\n",
      "epcoh nb:  24 loss:  tensor([35.7748])\n",
      "epcoh nb:  25 loss:  tensor([35.7739])\n",
      "epcoh nb:  26 loss:  tensor([35.7731])\n",
      "epcoh nb:  27 loss:  tensor([35.7722])\n",
      "epcoh nb:  28 loss:  tensor([35.7714])\n",
      "epcoh nb:  29 loss:  tensor([35.7705])\n",
      "epcoh nb:  30 loss:  tensor([35.7697])\n",
      "epcoh nb:  31 loss:  tensor([35.7688])\n",
      "epcoh nb:  32 loss:  tensor([35.7680])\n",
      "epcoh nb:  33 loss:  tensor([35.7671])\n",
      "epcoh nb:  34 loss:  tensor([35.7663])\n",
      "epcoh nb:  35 loss:  tensor([35.7654])\n",
      "epcoh nb:  36 loss:  tensor([35.7646])\n",
      "epcoh nb:  37 loss:  tensor([35.7637])\n",
      "epcoh nb:  38 loss:  tensor([35.7628])\n",
      "epcoh nb:  39 loss:  tensor([35.7620])\n",
      "epcoh nb:  40 loss:  tensor([35.7612])\n",
      "epcoh nb:  41 loss:  tensor([35.7603])\n",
      "epcoh nb:  42 loss:  tensor([35.7595])\n",
      "epcoh nb:  43 loss:  tensor([35.7586])\n",
      "epcoh nb:  44 loss:  tensor([35.7578])\n",
      "epcoh nb:  45 loss:  tensor([35.7569])\n",
      "epcoh nb:  46 loss:  tensor([35.7561])\n",
      "epcoh nb:  47 loss:  tensor([35.7552])\n",
      "epcoh nb:  48 loss:  tensor([35.7544])\n",
      "epcoh nb:  49 loss:  tensor([35.7536])\n",
      "epcoh nb:  50 loss:  tensor([35.7527])\n",
      "epcoh nb:  51 loss:  tensor([35.7518])\n",
      "epcoh nb:  52 loss:  tensor([35.7510])\n",
      "epcoh nb:  53 loss:  tensor([35.7501])\n",
      "epcoh nb:  54 loss:  tensor([35.7493])\n",
      "epcoh nb:  55 loss:  tensor([35.7485])\n",
      "epcoh nb:  56 loss:  tensor([35.7476])\n",
      "epcoh nb:  57 loss:  tensor([35.7468])\n",
      "epcoh nb:  58 loss:  tensor([35.7459])\n",
      "epcoh nb:  59 loss:  tensor([35.7451])\n",
      "epcoh nb:  60 loss:  tensor([35.7443])\n",
      "epcoh nb:  61 loss:  tensor([35.7434])\n",
      "epcoh nb:  62 loss:  tensor([35.7426])\n",
      "epcoh nb:  63 loss:  tensor([35.7417])\n",
      "epcoh nb:  64 loss:  tensor([35.7409])\n",
      "epcoh nb:  65 loss:  tensor([35.7400])\n",
      "epcoh nb:  66 loss:  tensor([35.7392])\n",
      "epcoh nb:  67 loss:  tensor([35.7384])\n",
      "epcoh nb:  68 loss:  tensor([35.7375])\n",
      "epcoh nb:  69 loss:  tensor([35.7367])\n",
      "epcoh nb:  70 loss:  tensor([35.7358])\n",
      "epcoh nb:  71 loss:  tensor([35.7350])\n",
      "epcoh nb:  72 loss:  tensor([35.7342])\n",
      "epcoh nb:  73 loss:  tensor([35.7333])\n",
      "epcoh nb:  74 loss:  tensor([35.7325])\n",
      "epcoh nb:  75 loss:  tensor([35.7317])\n",
      "epcoh nb:  76 loss:  tensor([35.7308])\n",
      "epcoh nb:  77 loss:  tensor([35.7300])\n",
      "epcoh nb:  78 loss:  tensor([35.7291])\n",
      "epcoh nb:  79 loss:  tensor([35.7283])\n",
      "epcoh nb:  80 loss:  tensor([35.7275])\n",
      "epcoh nb:  81 loss:  tensor([35.7266])\n",
      "epcoh nb:  82 loss:  tensor([35.7258])\n",
      "epcoh nb:  83 loss:  tensor([35.7249])\n",
      "epcoh nb:  84 loss:  tensor([35.7241])\n",
      "epcoh nb:  85 loss:  tensor([35.7233])\n",
      "epcoh nb:  86 loss:  tensor([35.7224])\n",
      "epcoh nb:  87 loss:  tensor([35.7216])\n",
      "epcoh nb:  88 loss:  tensor([35.7208])\n",
      "epcoh nb:  89 loss:  tensor([35.7199])\n",
      "epcoh nb:  90 loss:  tensor([35.7191])\n",
      "epcoh nb:  91 loss:  tensor([35.7183])\n",
      "epcoh nb:  92 loss:  tensor([35.7174])\n",
      "epcoh nb:  93 loss:  tensor([35.7166])\n",
      "epcoh nb:  94 loss:  tensor([35.7157])\n",
      "epcoh nb:  95 loss:  tensor([35.7149])\n",
      "epcoh nb:  96 loss:  tensor([35.7141])\n",
      "epcoh nb:  97 loss:  tensor([35.7133])\n",
      "epcoh nb:  98 loss:  tensor([35.7124])\n",
      "epcoh nb:  99 loss:  tensor([35.7116])\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "batch_size = 50\n",
    "for e in range(nb_epochs):\n",
    "    loss_acc=0\n",
    "    for b in range(0, 500, batch_size):\n",
    "        for i in range(batch_size):\n",
    "            prediction = model.forward(train_input[b+i])\n",
    "            loss_acc += loss.forward(prediction, train_target[b+i])\n",
    "            model.backward(loss.backward(prediction, train_target[b+i]))\n",
    "        model.SGD_step(0.0002)\n",
    "    print('epcoh nb: ', e, 'loss: ', loss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df6654de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "#Here we compute the accuracy:\n",
    "tot_right=0\n",
    "for i in range(500):\n",
    "    #print(model.forward(train_input[i]))\n",
    "    #print(train_target[i])\n",
    "    if model.forward(test_input[i]) <0.0 and  test_target[i]==0:\n",
    "        tot_right+=1\n",
    "    elif model.forward(test_input[i]) >=0.0 and  test_target[i]==1:\n",
    "        tot_right+=1\n",
    "print(tot_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd83e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3026f0c7",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "596f80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################### DATA things ################################\n",
    "######################################################################\n",
    "def generate_disc_set(nb): # This can be kept the same\n",
    "    input = torch.empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################### ERROR things ################################\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target):\n",
    "    tot_right = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        for i in range(mini_batch_size):\n",
    "            output = model.forward(data_input[b+i])\n",
    "            if output <0.0 and  data_target[b+i]==0:\n",
    "                tot_right+=1\n",
    "            elif output >=0.0 and  data_target[b+i]==1:\n",
    "                tot_right+=1\n",
    "\n",
    "    return tot_right/data_input.size(0)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################### TRAIN things ################################\n",
    "######################################################################\n",
    "#Training and Testing\n",
    "def train_model(model, train_input, train_target):\n",
    "    nb_epochs = 250\n",
    "    batch_size = 50\n",
    "    for e in range(nb_epochs):\n",
    "        loss_acc=0\n",
    "        for b in range(0, 1000, batch_size):\n",
    "            for i in range(batch_size):\n",
    "                prediction = model.forward(train_input[b+i])\n",
    "                loss_acc += loss.forward(prediction, train_target[b+i])\n",
    "                model.backward(loss.backward(prediction, train_target[b+i]))\n",
    "            model.SGD_step(0.0005)\n",
    "        if e%5 == 0:\n",
    "            print('epcoh nb: ', e, 'loss: ', loss_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d3d96826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = framework.Sequential((framework.Linear(2, 10), \n",
    "                              framework.Tanh(), \n",
    "                              framework.Linear(10, 10), \n",
    "                              framework.Tanh(),\n",
    "                              framework.Linear(10, 50), \n",
    "                              framework.Tanh(),\n",
    "                              framework.Linear(50, 1)))\n",
    "\n",
    "loss = framework.MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "214ec8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh nb:  0 loss:  tensor([332.4319])\n",
      "epcoh nb:  5 loss:  tensor([254.9257])\n",
      "epcoh nb:  10 loss:  tensor([248.7937])\n",
      "epcoh nb:  15 loss:  tensor([238.6078])\n",
      "epcoh nb:  20 loss:  tensor([215.4829])\n",
      "epcoh nb:  25 loss:  tensor([170.4130])\n",
      "epcoh nb:  30 loss:  tensor([121.7979])\n",
      "epcoh nb:  35 loss:  tensor([130.3188])\n",
      "epcoh nb:  40 loss:  tensor([102.8324])\n",
      "epcoh nb:  45 loss:  tensor([92.8497])\n",
      "epcoh nb:  50 loss:  tensor([88.2676])\n",
      "epcoh nb:  55 loss:  tensor([85.8224])\n",
      "epcoh nb:  60 loss:  tensor([84.3027])\n",
      "epcoh nb:  65 loss:  tensor([83.2431])\n",
      "epcoh nb:  70 loss:  tensor([82.4418])\n",
      "epcoh nb:  75 loss:  tensor([81.8002])\n",
      "epcoh nb:  80 loss:  tensor([81.2650])\n",
      "epcoh nb:  85 loss:  tensor([80.8055])\n",
      "epcoh nb:  90 loss:  tensor([80.4018])\n",
      "epcoh nb:  95 loss:  tensor([80.0410])\n",
      "epcoh nb:  100 loss:  tensor([79.7142])\n",
      "epcoh nb:  105 loss:  tensor([79.4148])\n",
      "epcoh nb:  110 loss:  tensor([79.1381])\n",
      "epcoh nb:  115 loss:  tensor([78.8805])\n",
      "epcoh nb:  120 loss:  tensor([78.6388])\n",
      "epcoh nb:  125 loss:  tensor([78.4111])\n",
      "epcoh nb:  130 loss:  tensor([78.1953])\n",
      "epcoh nb:  135 loss:  tensor([77.9900])\n",
      "epcoh nb:  140 loss:  tensor([77.7938])\n",
      "epcoh nb:  145 loss:  tensor([77.6057])\n",
      "epcoh nb:  150 loss:  tensor([77.4246])\n",
      "epcoh nb:  155 loss:  tensor([77.2500])\n",
      "epcoh nb:  160 loss:  tensor([77.0808])\n",
      "epcoh nb:  165 loss:  tensor([76.9167])\n",
      "epcoh nb:  170 loss:  tensor([76.7571])\n",
      "epcoh nb:  175 loss:  tensor([76.6014])\n",
      "epcoh nb:  180 loss:  tensor([76.4492])\n",
      "epcoh nb:  185 loss:  tensor([76.3001])\n",
      "epcoh nb:  190 loss:  tensor([76.1538])\n",
      "epcoh nb:  195 loss:  tensor([76.0100])\n",
      "epcoh nb:  200 loss:  tensor([75.8682])\n",
      "epcoh nb:  205 loss:  tensor([75.7283])\n",
      "epcoh nb:  210 loss:  tensor([75.5901])\n",
      "epcoh nb:  215 loss:  tensor([75.4533])\n",
      "epcoh nb:  220 loss:  tensor([75.3177])\n",
      "epcoh nb:  225 loss:  tensor([75.1831])\n",
      "epcoh nb:  230 loss:  tensor([75.0493])\n",
      "epcoh nb:  235 loss:  tensor([74.9164])\n",
      "epcoh nb:  240 loss:  tensor([74.7838])\n",
      "epcoh nb:  245 loss:  tensor([74.6518])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "# center the data:\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "\n",
    "train_model(model, train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6e266bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03771a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a7822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe6462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6fe29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae2a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cf10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f345f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a42fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76670a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c47523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af3971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d30836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee023e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf45cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42d0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8610e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4036a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99eaf80e",
   "metadata": {},
   "source": [
    "#Below this point is previous practicals ad drafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9876a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem Set 3:\n",
    "#/usr/bin/env python\n",
    "\n",
    "# Any copyright is dedicated to the Public Domain.\n",
    "# https://creativecommons.org/publicdomain/zero/1.0/\n",
    "\n",
    "# Written by Francois Fleuret <francois@fleuret.org>\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def sigma(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dsigma(x):\n",
    "    return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def loss(v, t): #This is MSE, right?\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def forward_pass(w1, b1, w2, b2, x):\n",
    "    x0 = x\n",
    "    s1 = w1.mv(x0) + b1\n",
    "    x1 = sigma(s1)\n",
    "    s2 = w2.mv(x1) + b2\n",
    "    x2 = sigma(s2)\n",
    "\n",
    "    return x0, s1, x1, s2, x2\n",
    "\n",
    "#whose arguments correspond to the networkâ€™s parameters, \n",
    "#the target vector, the quantities computed by the forward pass, \n",
    "#and the tensors used to store the cumulated sums of the gradient \n",
    "#on individual samples, and updates the latters according to the\n",
    "#formula of the backward pass.\n",
    "\n",
    "def backward_pass(w1, b1, w2, b2, #current weights and biases of the network\n",
    "                  t, # target vector\n",
    "                  x, s1, x1, s2, x2, #output of network\n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2): #tensors used to stor the sumulated sums of the gradient\n",
    "    x0 = x\n",
    "    dl_dx2 = dloss(x2, t)\n",
    "    dl_ds2 = dsigma(s2) * dl_dx2\n",
    "    dl_dx1 = w2.t().mv(dl_ds2)\n",
    "    dl_ds1 = dsigma(s1) * dl_dx1\n",
    "\n",
    "    dl_dw2.add_(dl_ds2.view(-1, 1).mm(x1.view(1, -1)))\n",
    "    dl_db2.add_(dl_ds2)\n",
    "    dl_dw1.add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db1.add_(dl_ds1)\n",
    "\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a3a5b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1673c8443e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mzeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "nb_classes = train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "train_target = train_target * zeta\n",
    "test_target = test_target * zeta\n",
    "\n",
    "nb_hidden = 50\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "w1 = torch.empty(nb_hidden, train_input.size(1)).normal_(0, epsilon)\n",
    "b1 = torch.empty(nb_hidden).normal_(0, epsilon)\n",
    "w2 = torch.empty(nb_classes, nb_hidden).normal_(0, epsilon)\n",
    "b2 = torch.empty(nb_classes).normal_(0, epsilon)\n",
    "\n",
    "dl_dw1 = torch.empty(w1.size())\n",
    "dl_db1 = torch.empty(b1.size())\n",
    "dl_dw2 = torch.empty(w2.size())\n",
    "dl_db2 = torch.empty(b2.size())\n",
    "\n",
    "for k in range(1000):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    dl_dw1.zero_()\n",
    "    dl_db1.zero_()\n",
    "    dl_dw2.zero_()\n",
    "    dl_db2.zero_()\n",
    "\n",
    "    for n in range(nb_train_samples):\n",
    "        x0, s1, x1, s2, x2 = forward_pass(w1, b1, w2, b2, train_input[n])\n",
    "\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if train_target[n, pred] < 0.5: nb_train_errors = nb_train_errors + 1\n",
    "        acc_loss = acc_loss + loss(x2, train_target[n])\n",
    "\n",
    "        backward_pass(w1, b1, w2, b2,\n",
    "                      train_target[n],\n",
    "                      x0, s1, x1, s2, x2,\n",
    "                      dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "\n",
    "    # Gradient step\n",
    "\n",
    "    w1 = w1 - eta * dl_dw1\n",
    "    b1 = b1 - eta * dl_db1\n",
    "    w2 = w2 - eta * dl_dw2\n",
    "    b2 = b2 - eta * dl_db2\n",
    "\n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(test_input.size(0)):\n",
    "        _, _, _, _, x2 = forward_pass(w1, b1, w2, b2, test_input[n])\n",
    "\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if test_target[n, pred] < 0.5: nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae105424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std create_shallow_model -1.000000 train_error 1.00% test_error 0.50%\n",
      "std create_deep_model -1.000000 train_error 8.50% test_error 8.90%\n",
      "std create_shallow_model 0.001000 train_error 1.40% test_error 1.60%\n",
      "std create_deep_model 0.001000 train_error 49.60% test_error 50.00%\n",
      "std create_shallow_model 0.010000 train_error 0.70% test_error 1.40%\n",
      "std create_deep_model 0.010000 train_error 49.60% test_error 50.00%\n",
      "std create_shallow_model 0.100000 train_error 1.00% test_error 0.50%\n",
      "std create_deep_model 0.100000 train_error 50.40% test_error 50.00%\n",
      "std create_shallow_model 1.000000 train_error 0.80% test_error 0.50%\n",
      "std create_deep_model 1.000000 train_error 50.40% test_error 50.00%\n",
      "std create_shallow_model 10.000000 train_error 0.60% test_error 0.80%\n",
      "std create_deep_model 10.000000 train_error 50.40% test_error 50.00%\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True) #At the end, this should be set to False\n",
    "\n",
    "#Pb set 5:\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Any copyright is dedicated to the Public Domain.\n",
    "# https://creativecommons.org/publicdomain/zero/1.0/\n",
    "\n",
    "# Written by Francois Fleuret <francois@fleuret.org>\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def generate_disc_set(nb):\n",
    "    input = torch.empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "#train_input, train_target = generate_disc_set(1000)\n",
    "#test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def train_model(model, train_input, train_target):\n",
    "    criterion = nn.CrossEntropyLoss() #replace this by self macde MSE\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1) #replace this by self made SGD for MSE\n",
    "    nb_epochs = 250\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad() # replace this so it works with \n",
    "            loss.backward() # replace this to self made backard method\n",
    "            optimizer.step() # '_'\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target): #this s\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )\n",
    "\n",
    "def create_deep_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "\n",
    "for std in [ -1, 1e-3, 1e-2, 1e-1, 1e-0, 1e1 ]:\n",
    "\n",
    "    for m in [ create_shallow_model, create_deep_model ]:\n",
    "\n",
    "        model = m()\n",
    "\n",
    "        if std > 0:\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p.normal_(0, std)\n",
    "\n",
    "        train_model(model, train_input, train_target)\n",
    "\n",
    "        print('std {:s} {:f} train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "            m.__name__,\n",
    "            std,\n",
    "            compute_nb_errors(model, train_input, train_target) / train_input.size(0) * 100,\n",
    "            compute_nb_errors(model, test_input, test_target) / test_input.size(0) * 100\n",
    "        )\n",
    "        )\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa7590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
